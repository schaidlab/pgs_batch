# pgs_batch

A script to run PGS from the PGS Catalog in batches using the `nextflow` pipeline `pgsc_calc` (v2.0.0-alpha.5).
Uses scoring files from the PGS Catalog downloaded on 2024/05/10 (n=4735).

## Dependencies
- java v8+
	- If having trouble with java when running the scripts below, export these variables `export JAVA_HOME=/path/to/java` and `export NXF_JAVA_HOME=/path/to/java`
- nextflow
- R>=4.2 and Rscript

## Setup

```bash
git clone https://github.com/krillinor/pgs_batch.git
cd pgs_batch
# install nextflow
curl -fsSL get.nextflow.io | bash
# install R packages
Rscript -e 'install.packages(c("docopt", "data.table", "fs", "readr", "curl", "stringr"), repos = "http://cran.us.r-project.org")'
```

## Config file

Change memory/cpus/max_forks in the `custom.config` file (details on nextflow/pgsc_calc config [here](https://pgsc-calc.readthedocs.io/en/latest/how-to/bigjob.html#how-do-i-run-pgsc-calc-on-larger-datasets-and-more-powerful-computers)).

## Output

Results will appear in the `results` directory.
Takes ~1hr to run for 500 PGS (1/10 batches) with the default config file.
The `runs` directory is temporary and takes a lot of space. Best to remove subdirectories in `runs` after PGS have been computed.

## Example

### Step 1: Specify number of batches

Create batches based on available scoring files on 2024/05/10 (n=4735).
For example, 10 batches (~500 per file).

```bash
Rscript pgs_batch.R batch --n_batches=10
```

### Step 2: Download scoring files

Download scoring files for each batch.
Specifying the correct build for the target cohort is important (GRCh37/GRCh38).
Use `--resume` if the download fails.

```bash
for i in {1..10}; do
    Rscript pgs_batch.R download --batch_id=${i} --target_build=GRCh37
done
```

### Step 3: Create the samplesheet

Specify the inputs.
You need to provide the cohort name with `--id` (replace "cohort_name" with the name of your cohort, f.x., "UKB"), the prefix to the genotypes with `--genos_path_prefix`, and the genotype format (vcf/bfile/pfile) with `--format`. Assumes one file per chromosome. If there's a single genotype file, use `--genos_single_file`.
More details [here](https://pgsc-calc.readthedocs.io/en/latest/how-to/samplesheet.html#setup-samplesheet).

```bash
Rscript pgs_batch.R create_samplesheet --id=cohort_name --genos_path_prefix="/path/to/genotypes/prefix" --format=bfile
```

Creates the file `samplesheet_cohort_name.csv`:

```
sampleset,path_prefix,chrom,format
cohort_name,/path/to/genotypes/prefix1,1,bfile
cohort_name,/path/to/genotypes/prefix2,2,bfile
cohort_name,/path/to/genotypes/prefix3,3,bfile
cohort_name,/path/to/genotypes/prefix4,4,bfile
cohort_name,/path/to/genotypes/prefix5,5,bfile
cohort_name,/path/to/genotypes/prefix6,6,bfile
cohort_name,/path/to/genotypes/prefix7,7,bfile
cohort_name,/path/to/genotypes/prefix8,8,bfile
cohort_name,/path/to/genotypes/prefix9,9,bfile
cohort_name,/path/to/genotypes/prefix10,10,bfile
cohort_name,/path/to/genotypes/prefix11,11,bfile
cohort_name,/path/to/genotypes/prefix12,12,bfile
cohort_name,/path/to/genotypes/prefix13,13,bfile
cohort_name,/path/to/genotypes/prefix14,14,bfile
cohort_name,/path/to/genotypes/prefix15,15,bfile
cohort_name,/path/to/genotypes/prefix16,16,bfile
cohort_name,/path/to/genotypes/prefix17,17,bfile
cohort_name,/path/to/genotypes/prefix18,18,bfile
cohort_name,/path/to/genotypes/prefix19,19,bfile
cohort_name,/path/to/genotypes/prefix20,20,bfile
cohort_name,/path/to/genotypes/prefix21,21,bfile
cohort_name,/path/to/genotypes/prefix22,22,bfile
```

### Step 4: Run pgsc_calc for each batch

Run sequentially for each batch (or submit to cluster).
Specifying the correct build for the target cohort is important (GRCh37/GRCh38).
Specify docker/singularity/conda with `--profile` based on availability.
Modify the `custom.config` file to use more/less resources (f.x., if killed because of too little memory).
Use `--resume` to resume the pipeline from the last completed step (f.x., if it failed somewhere because of too little memory).
Use `--extra_args` to specify additional parameters, f.x., `--extra_args="  --keep_ambiguous"` to keep ambiguous variants (not recommended) in the matching step (`pgsc_calc` automatically filters them out).

```bash
for i in {1..10}; do
    Rscript pgs_batch.R calc --id=cohort_name --target_build=GRCh37 --batch_id=${i} --profile=docker
done
```

## Offline environment

If working in an offline environment, and do the following steps in an online environment before moving to the offline environment transfer.

1. Download all scoring files (see Step 2 in the example below).
2. Download `pgsc_calc` and plugins:

```bash
# download pgsc_calc and unzip
wget https://github.com/PGScatalog/pgsc_calc/archive/refs/tags/v2.0.0-alpha.5.zip
unzip v2.0.0-alpha.5.zip

export NXF_HOME="${PWD}/.nextflow"
./nextflow plugin install nf-validation@1.1.3
```

3. Download `singularity` containers (also possible for [docker](https://pgsc-calc.readthedocs.io/en/latest/how-to/offline.html#docker)):

```bash
cd pgsc_calc-2.0.0-alpha.5
NXF_SINGULARITY_CACHEDIR=nxf_sc
mkdir -p $NXF_SINGULARITY_CACHEDIR
grep 'ext.singularity*' conf/modules.config | cut -f 2 -d '=' | xargs -L 2 echo | tr -d ' ' > singularity_images.txt
cat singularity_images.txt | sed 's/oras:\/\///;s/https:\/\///;s/\//-/g;s/$/.img/;s/:/-/' > singularity_image_paths.txt
paste -d '\n' singularity_image_paths.txt singularity_images.txt | xargs -L 2 sh -c 'singularity pull --disable-cache --dir $NXF_SINGULARITY_CACHEDIR $0 $1'
```

4. Move everything (the pgs_batch directory) to the offline environment.
5. Use `--offline` when running `Rscript pgs_batch.R calc`.

## Computing cluster

If using a computing cluster, specify the details in `custom.config` (example for `slurm`):

```
process {
    executor = 'slurm'
    queue    = 'cpu-short'

    withLabel:process_low {
        cpus   = 2
        memory = 8.GB
        time    = 1.h
    }
    withLabel:process_medium {
        cpus     = 4
        memory   = 16.GB
        maxForks = 4
        time     = 4.h
    }
    withName: PLINK2_SCORE {
        maxForks = 22
    }
}

executor {
    name = 'slurm'
    queueSize = 22
    submitRateLimit = '10 sec'
}
```

Also, export these variables in the `sbatch` script:

```
export NXF_ANSI_LOG=false
export NXF_OPTS="-Xms1G -Xmx4G"
```

## TODOs

- arg checks
- combine results + QC metrics
- delete runs subdirs on completion unless flag
